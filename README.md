# DiSCo: Disentangled Attribute Manipulation Retrieval via Semantic Reconstruction and Consistency Regularization

## Installation 

Follow the [instructions](https://www.anaconda.com/products/individual) to install Anaconda. 
You can use python venv if preferred.

```bash
conda create -n disco python=3.9.16
conda activate disco
pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116
pip install -r requirements.txt
```

## Data Preparation
We use the following publicly-available datasets that you will need to download from the original sources:
+ **Shopping100k**: contact [the author of the dataset](https://sites.google.com/view/kenanemirak/home) to get access to the images.
+ **DeepFashion**: download images and labels for the category and attribute prediction benchmark from [the dataset website](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/AttributePrediction.html).


## training
train the basic feature extraction network CCA using the script `1_run_encoder.sh`.

train the feature manipulation network. This network consists of the CCA feature extractor from Step 1 and the feature modification module AAC. Train it using the script `2_run_manip_cca.sh`.

Train a GAN model to directly generate samples that meet the modification requirements. This GAN needs to be trained under the supervision of the attribute classifier, specifically the CCA model obtained after Step 2. Use the script `3_run_gan.sh` for training.

## Evaluation
After training is complete, you can evaluate the model's retrieval performance using the `script 4_run_eval_joint.sh` . In this script, the execution parameter fusion_weight controls the strength of the information-enhanced retrieval:

1. When set to `0`, it uses only the feature manipulation network from Step 2.

2. When set to `1`, it uses only the samples generated by the GAN generator.

Other values represent fusing the features from both networks, further enhancing the model's performance.



## Preparing Support Files

We describe here how the files in the `splits` folder were prepared so one can run experiments on a different dataset.

### Files for Training

The dataset need to be pre-processed to generate train/test split set, triplets and queries. 
These are the files that one would need to create:
+ `imgs_train.txt`,`imgs_test.txt`: they store the name of images in relative path for training and testing.
+ `labels_train.txt`,`labels_test.txt`: they store one-hot attribute labels, used to train the attribute-driven disentangled encoder. 
`labels_*.txt` is paired with `imgs_*.txt`, that is, the i-th line in  `labels_*.txt`  is the label vector for the i-th line image in `img_*.txt`.
+ `attr_num.txt`: a list that consists of the number of attribute values for each attribute type.

   For example in Shopping100k, we list is `[16, 17, 19, 14, 10, 15, 2, 11, 16, 7, 9, 15]` since there are 12 attribute types in total, and the first attribute (category) has 16 values, the second has 17 and so on.

The triplets for attribute manipulation are generated offline.
For each triplet, we create the indicator vector by randomly selecting an attribute type and a new attribute value for the attribute.
Then based on the indicator vector, we randomly pick the images with all target attributes as positive samples, 
and randomly pick images that have different attributes as negative samples.  We should finally generate the two paired files:
+ `triplet_train.txt`: each line corresponds to a triplet. A triplet is defined by 3 space-separated indexes (reference, positive, negative). The index of images is the line index of the file `labels_train.txt`.

   E.g. each line should be `reference_id positive_id  negative_id`
+ `triplet_train_ind.txt`: each line is the indicator vector for each triplets. Each number in a line is 1, 0, or -1 if the attribute is changed but in the reference, preserved or changed but in the target image, respectively.

### Files for Evaluation Queries

We enumerate all attribute types and all different attribute values to generate target attributes,
and if there is image has the target attributes, the query is valid. We generate the following paired files:
+ `ref_test.txt`: 1-D list that consists of indexes (line index in `labels_test.txt`) of reference images.
+ `indfull_test.txt`: array that consists of the full indicator vector (same dimension as the merged one-hot label vector) for each reference image.
+ `gt_test.txt`: array that stores ground truth target labels for each query.

For the i-th query, the i-th line in `ref_test.txt` indicates the index of reference images in the test set, 
the i-th line in `indfull_test.txt` is the indicate vector consists of -1, 0, 1,
and the i-th line in `gt_test.txt` is the one-hot label vectors of the target attributes.


